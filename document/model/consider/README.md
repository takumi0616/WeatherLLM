# 視覚言語モデル（Image-Text-to-Text）の概要

Hugging Face のモデルハブでは、**画像やテキスト入力からテキストを生成する** Vision-Language Models (VLM：視覚言語モデル) が数多く公開されています ([huggingface.co](https://huggingface.co/docs/transformers/main/tasks/image_text_to_text#:~:text=Image))。これらは「画像からのキャプション生成」や「ビジュアル質問応答（VQA）」「文書画像のレイアウト解析」など幅広いタスクに対応します。以下にパラメータ数がおおよそ 10 億～ 100 億以下の代表的なモデルをいくつか紹介します（※パラメータ数はモデルカード記載値）。

- **YannQi/R-4B**（約 4.8B パラメータ）: 自動的にステップ思考（auto-thinking）モードと通常モードを切り替えることで高品質な応答を狙う最新のマルチモーダル LLM です。画像＋テキスト入力に対応し、視覚情報に基づく質問応答や画像説明など一般的な VQA タスクで国際ベンチマークをリードします ([huggingface.co](https://huggingface.co/YannQi/R-4B#:~:text=Model%20size))。
- **openbmb/MiniCPM-V-4_5**（約 8B）: 画像・動画理解に特化したモデルで、高 FPS 動画の解析や複数画像の理解が可能です。基本モデルは「Qwen3-8B」と「SigLIP2-400M」を組み合わせた構成で、8B パラメータながら視覚言語能力テスト (OpenCompass) で高スコアを記録。GPT-4o など大規模商业モデルにも迫る性能を持ちます ([huggingface.co](https://huggingface.co/openbmb/MiniCPM-V-4_5#:~:text=model%20is%20built%20on%20Qwen3,V%204.5%20include))。
- **rednote-hilab/dots.ocr**（約 3.0B）: 文書画像（OCR）向けの視覚言語モデルです。多言語文書のレイアウト解析や段組み検出、テキスト認識を一つのモデルでこなします。DocAI 向けに設計され、画像中の文字・表・数式などを構造化されたテキスト形式で出力できる点が特徴です（モデルサイズは約 3.04B） ([huggingface.co](https://huggingface.co/rednote-hilab/dots.ocr#:~:text=Safetensors))。
- **Qwen/Qwen2.5-VL-7B-Instruct**（約 8.3B）: アリババ系の Qwen シリーズによる視覚言語モデル（インストラクト版）です。7B（正確には 8.29B）規模の大規模言語モデルをバックボーンとし、画像とテキストの入力から指示応答を生成できます ([huggingface.co](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct#:~:text=Model%20size))。Tencent 社のベンチマークで GPT-4 レベルの成績を示すなど、中国語圏で注目されています。
- **microsoft/kosmos-2.5**（約 1.4B）: 文書など**文字情報が主体の画像**を解析するためのモデルです。画像中のテキストブロックを空間座標つきで認識し、Markdown 形式など構造化されたテキスト出力が可能です ([huggingface.co](https://huggingface.co/microsoft/kosmos-2.5#:~:text=Model%20size))。小規模（1.37B）ながら OCR 後処理や文書理解のタスクを統一的に扱える点が特徴です。
- **Qwen/Qwen2.5-VL-3B-Instruct**（約 3.8B）: 上記 Qwen2.5-VL の小型版モデルです（3.75B）。同じく Vision-LM のインストラクション版で、軽量化されているぶん高速に動作しますが、各種視覚テキストタスクに対応できる点は同様です ([huggingface.co](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct#:~:text=Model%20size))。
- **LiquidAI/LFM2-VL-1.6B**（約 1.6B）: エッジデバイス向けに最適化された低遅延視覚言語モデルです。Liquid AI 社の LFM2 系バックボーンを用い、GPU での推論速度が 2 倍となるよう設計されています ([huggingface.co](https://huggingface.co/LiquidAI/LFM2-VL-1.6B#:~:text=Model%20size))。1.58B パラメータと比較的小規模であり、モバイル/組み込み機器での画像キャプションや簡易 VQA に向いています。
- **ds4sd/SmolDocling-256M**（約 0.26B）: ドキュメント変換向けに超軽量化されたモデルです。文書画像からテキストを抽出して整形する「ドキュメント・リンギング」タスクに特化し、256M パラメータながら Docling 互換の高度な文書 OCR 処理を実現します ([huggingface.co](https://huggingface.co/ds4sd/SmolDocling-256M-preview#:~:text=match%20at%20L416%20256M%20params))。超小型でリソース制約のある環境でも動作可能です。
- **AIDC-AI/Ovis2.5-9B**（約 9.2B）: 画像の「原寸（ネイティブ解像度）」処理に対応したモデルです。NaViT (Native-Resolution Vision Transformer) を搭載し、元の解像度のまま視覚情報を取り込めるため、細部の判別が必要な画像理解や複雑な推論に強みがあります ([huggingface.co](https://huggingface.co/AIDC-AI/Ovis2.5-9B#:~:text=Model%20size))。パラメータは 9.17B で、マルチモーダル QA や画像分類など柔軟にこなします。
- **LLaVA（llava-hf/llava-1.5-7b-hf）**（約 7.1B）: LLaMA/Vicuna を視覚タスクで強化したモデルです。GPT 生成のマルチモーダル指示データでファインチューニングされたチャット型モデルであり、7.06B パラメータを持つ ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=Model%20size))。画像を見せながら質問すると回答できる、いわゆる「ビジョン・チャットアシスタント」です ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=Model%20type%3A%20LLaVA%20is%20an,based%20on%20the%20transformer%20architecture))。
- **Salesforce/BLIP-2 (OPT-2.7b)**（約 2.7B）: いわずと知れた BLIP-2 モデルです。視覚エンコーダ（CLIP ライク）と GPT 系の大規模言語モデル（ここでは OPT-2.7b）を組み合わせ、画像に対するテキスト生成を行います。OPT-2.7b 本体だけで 2.7B パラメータなので、Vision→Text タスクにおいてもその豊かな言語能力を活かせます ([huggingface.co](https://huggingface.co/paragon-AI/blip2-image-to-text#:~:text=BLIP,first%20released%20in%20this%20repository))。画像キャプションや VQA 用途で広く使われています。

以上のモデルはすべて Hugging Face モデルハブで公開されており、（無料登録すると）直接利用できます。それぞれ得意とするタスクや性能が異なるため、用途に応じて適切なモデルを選択すると良いでしょう。

**参考資料:** Hugging Face Transformers ドキュメント ([huggingface.co](https://huggingface.co/docs/transformers/main/tasks/image_text_to_text#:~:text=Image))および各モデルの公式ページ ([huggingface.co](https://huggingface.co/YannQi/R-4B#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/openbmb/MiniCPM-V-4_5#:~:text=model%20is%20built%20on%20Qwen3,V%204.5%20include)) ([huggingface.co](https://huggingface.co/rednote-hilab/dots.ocr#:~:text=Safetensors)) ([huggingface.co](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/microsoft/kosmos-2.5#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/LiquidAI/LFM2-VL-1.6B#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/ds4sd/SmolDocling-256M-preview#:~:text=match%20at%20L416%20256M%20params)) ([huggingface.co](https://huggingface.co/AIDC-AI/Ovis2.5-9B#:~:text=Model%20size)) ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=Model%20type%3A%20LLaVA%20is%20an,based%20on%20the%20transformer%20architecture)) ([huggingface.co](https://huggingface.co/paragon-AI/blip2-image-to-text#:~:text=BLIP,first%20released%20in%20this%20repository))など。各引用部はモデル説明やパラメータ数を示しています。

# 画像＋テキスト → テキスト変換モデルの例

画像とテキストを入力にとり、テキストを生成する「image-text-to-text」モデル（いわゆる視覚言語モデル）は、視覚質問応答（VQA）や画像キャプション生成など様々なタスクで用いられます ([huggingface.co](https://huggingface.co/docs/transformers/en/tasks/image_text_to_text#:~:text=Image,and%20are%20more%20generalist%20models))。以下に、Hugging Face 上で公開されている、パラメータサイズおよそ 10 億～ 100 億以下の代表的な画像＋テキスト → テキストモデルをいくつか挙げます。

- **Idefics2-8B**: 画像とテキストをインプットにとるオープンなマルチモーダルモデル。画像に関する質問応答や内容説明、複数画像に基づく物語生成などを行える ([huggingface.co](https://huggingface.co/docs/transformers/model_doc/idefics2#:~:text=Idefics2%20is%20an%20open%20multimodal,allows%20for%20varying%20inference%20efficiency))。パラメータ数は約 80 億（8B）と軽量で、オリジナル論文では同サイズ帯で高い性能を示しています ([huggingface.co](https://huggingface.co/docs/transformers/model_doc/idefics2#:~:text=Idefics2%20is%20an%20open%20multimodal,allows%20for%20varying%20inference%20efficiency))。
- **IDefics-9B**: DeepMind の Flamingo に相当するオープンアクセスモデル。名前は異なるものの、画像とテキストを組み合わせてテキストを出力する点で Flamingo と同様に動作します ([huggingface.co](https://huggingface.co/HuggingFaceM4/idefics-9b#:~:text=IDEFICS%20%28Image,publicly%20available%20data%20and%20models))。こちらは約 90 億（9B）パラメータの版が提供されており、命令調整された「idefics-9b-instruct」等も利用可能です ([huggingface.co](https://huggingface.co/HuggingFaceM4/idefics-9b#:~:text=choice,and%20a%20%203%20version))。
- **LLaVA-v1.5-7B**: LLaVA（Large Language and Vision Assistant）は、LLaMA/Vicuna をベースに GPT-4 風のマルチモーダル対話データでチューニングされたモデルです。バージョン 1.5-7B は約 70 億パラメータで、複数画像とテキストの対話的入力に対応します ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=Model%20type%3A%20LLaVA%20is%20an,based%20on%20the%20transformer%20architecture))。Hugging Face Transformers では `pipeline("image-text-to-text", model="llava-hf/llava-1.5-7b-hf")` のように呼び出して使用できます ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=))。
- **Qwen-VL-Chat (Qwen-7B)**: Alibaba が開発した「通義千問（Qwen）」シリーズのマルチモーダルモデル。ベースの言語モデルは 7B で、画像・テキスト・領域（バウンディングボックス）を入力として受け取り、対話形式でテキスト応答や画像上の物体検出（ボックス出力）を行います ([huggingface.co](https://huggingface.co/Qwen/Qwen-VL-Chat/blob/807926788c1456eda0d3ad6840f2858126d5b468/README.md#:~:text=%2A%20Qwen,is%20trained%20with%20alignment%20techniques))。Hugging Face では `AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)` などで利用可能です ([huggingface.co](https://huggingface.co/Qwen/Qwen-VL-Chat#:~:text=model%20%3D%20AutoModelForCausalLM.from_pretrained%28%22Qwen%2FQwen)) ([huggingface.co](https://huggingface.co/Qwen/Qwen-VL-Chat/blob/807926788c1456eda0d3ad6840f2858126d5b468/README.md#:~:text=%2A%20Qwen,is%20trained%20with%20alignment%20techniques))。
- **mPLUG-Owl3-7B**: 清華大学（Tsinghua University）開発のマルチモーダル大規模言語モデル。画像系列（長い画像列やビデオ）も効率的に処理できるよう工夫されており、単一画像から複数画像、動画まで幅広い視覚タスクに対応します ([huggingface.co](https://huggingface.co/mPLUG/mPLUG-Owl3-7B-241101#:~:text=mPLUG,image%2C%20and%20video%20tasks))。パラメータ数は約 80.7 億（8.07B）です ([huggingface.co](https://huggingface.co/mPLUG/mPLUG-Owl3-7B-241101#:~:text=8))。

これらのモデルはすべて Hugging Face 上に公開されており、各モデルのリポジトリやドキュメントから詳しい使用例を参照できます ([huggingface.co](https://huggingface.co/docs/transformers/model_doc/idefics2#:~:text=Idefics2%20is%20an%20open%20multimodal,allows%20for%20varying%20inference%20efficiency)) ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=))（例：Idefics2 なら `AutoModelForImageTextToText.from_pretrained("HuggingFaceM4/idefics2-8b")`）。

**参考資料:** 画像＋テキスト系モデルに関する公式ドキュメントやモデルカード ([huggingface.co](https://huggingface.co/docs/transformers/en/tasks/image_text_to_text#:~:text=Image,and%20are%20more%20generalist%20models)) ([huggingface.co](https://huggingface.co/docs/transformers/model_doc/idefics2#:~:text=Idefics2%20is%20an%20open%20multimodal,allows%20for%20varying%20inference%20efficiency)) ([huggingface.co](https://huggingface.co/HuggingFaceM4/idefics-9b#:~:text=IDEFICS%20%28Image,publicly%20available%20data%20and%20models)) ([huggingface.co](https://huggingface.co/llava-hf/llava-1.5-7b-hf#:~:text=Model%20type%3A%20LLaVA%20is%20an,based%20on%20the%20transformer%20architecture)) ([huggingface.co](https://huggingface.co/Qwen/Qwen-VL-Chat/blob/807926788c1456eda0d3ad6840f2858126d5b468/README.md#:~:text=%2A%20Qwen,is%20trained%20with%20alignment%20techniques)) ([huggingface.co](https://huggingface.co/mPLUG/mPLUG-Owl3-7B-241101#:~:text=8))（各モデルの詳細説明）。
